---
title: "supervised"
author: "kelvin njunge"
date: "9/3/2021"
output:
  pdf_document: supervised
  html_document: supervised
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# **Research Question**
A Kenyan entrepreneur has created an online cryptography course and would want to advertise it on her blog. She currently targets audiences originating from various countries. In the past, she ran ads to advertise a related course on the same blog and collected data in the process. She would now like to employ your services as a Data Science Consultant to help her identify which individuals are most likely to click on her ads.

## **1. Defining the question**

### **1.1  Specifying the data analytic objective**
Our main aim is to do thorough exploratory data analysis for univariate and bivariate and come up with recommendations for our client.

### **1.2 Defining the metric  for success**
We aim to do a elaborate  visualizations for univariate and bivariate anaysis.

### **1.3. Recording the experimental design**
* Loading the data
* Checking the data
* Tidying the data
* univariate analysis
* Bivariate analysis
* Challenge the solution
* Recommendation
* Follow up questions

### **1.4. Data Relevance**
The data provided was relevant for our analysis

## ***Loading the data***

```{r}
getwd()
```
```{r}
df <- read.csv("C:\\Users\\Ricky\\Documents\\Adobe\\advertising.csv")
```

## *** Checking the data***
### ***Viewing the top 6 entries***
```{r}
head(df)

```

### *** Viewing the bottom 6 entries***
```{r}
tail(df)

```
### *** viewingthe info of the dataset***
```{r}
str(df)

```

## *** 3. Tidying the data***
### ***Checking for null values***
```{r}
any(is.na(df))

```

### *** Checking for duplicates***
```{r}
sum(duplicated(df))

```
There are no missing values in our data.
There are no duplicates in our data.

### *** Checking for outliers***
```{r}
boxplot(df$Area.Income,main="Boxplot for Area.Income",col = "cyan")

```

```{r}
boxplot(df$Daily.Internet.Usage,main="Boxplot for Daily.Internet.Usage",col="brown")
```

```{r}

boxplot(df$Male,main="Boxplot for Male",col = "violet")
```

```{r}
boxplot(df$Age,main="Boxplot for Age",col = "orange")


```

```{r}
boxplot(df$Clicked.on.Ad,main="Boxplot for Clicked.on.Ad",col = "green")

```


Area income was the only column that had outliers

## ***4. Univariate analysis***
### *** 4.1. measures of central tendency ***
### ***4.1.1 mean***

```{r}
print("The mean for numeric variables is:")
colMeans(df[sapply(df,is.numeric)])
```

### ***4.1.2 median***
```{r}
cat("The median for daily time spent on site is",median(df$Daily.Time.Spent.on.Site))
cat("\n")
```

```{r}
cat("The median for age is",median(df$Age))
cat("\n")
```

```{r}
cat("The median for Area.Income is",median(df$Area.Income))
cat("\n")
```

```{r}
cat("The median for daily Internet Usage is",median(df$Daily.Internet.Usage))
cat("\n")
```

```{r}
cat("The median for Clicked on Ad",median(df$Clicked.on.Ad))

cat("\n")
```

### ***4.1.3 mode***
```{r}
#Creating a function for the mode
getmode <- function(v) {
   uniqv <- unique(v)
   uniqv[which.max(tabulate(match(v, uniqv)))]
}
```
```{r}
cat("The mode for daily time spent on site is",getmode(df$Daily.Time.Spent.on.Site))
cat("\n")
```
```{r}
cat("The mode for age is",getmode(df$Age))
cat("\n")
```
```{r}
cat("The mode for Area.Income is",getmode(df$Area.Income))
cat("\n")
```

```{r}
cat("The mode for daily Internet Usage is",getmode(df$Daily.Internet.Usage))
cat("\n")
```

```{r}
cat("The mode for Clicked on Ad",getmode(df$Clicked.on.Ad))
cat("\n")
```

### *** 4.1.4 standard deviation ***
```{r}
cat("The standard deviation for age is",sd(df$`Age`))
cat("\n")
```
```{r}
cat("The standard deviation for Area.Income is",sd(df$`Area Income`))
cat("\n")
```
```{r}
cat("The mode for daily time spent on site is",sd(df$Daily.Time.Spent.on.Site))
cat("\n")
```


### *** 4.1.5. variance***
```{r}
cat("The variance for daily time spent on site is",var(df$Daily.Time.Spent.on.Site))
cat("\n")
```
```{r}
cat("The variance for age is",var(df$Age))

```

```{r}
cat("The variance for daily Internet Usage is",var(df$Daily.Internet.Usage))
```
```{r}
cat("The variance for Clicked on Ad",var(df$Clicked.on.Ad))
```

```{r}
cat("The variance for Area.Income is",var(df$Area.Income))
```

```{r}
cat("The variance for male is",var(df$Male))
```

### ***4.2.2 maximum***
```{r}
library(dplyr)
df %>% summarise_if(is.numeric, max)

```

### *** minimum of the columns *** 
```{r}
df %>% summarise_if(is.numeric,min)
```

### *** 4.2.3 Range ***
```{r}
cat("The range for daily time spent on site is",range(df$Daily.Time.Spent.on.Site))
```
```{r}
cat("The range for age is",range(df$Age))
```

```{r}
cat("The range for Area income is",range(df$Area.Income))
```

```{r}
cat("The range for male is",range(df$Male))
```

```{r}
cat("The range  for daily internet usage is,",range(df$Daily.Internet.Usage))
```
```{r}
cat("The range for clicked on ad",range(df$Clicked.on.Ad))
```

### *** 4.2.3 Quantile ***
```{r}

cat("The Quantile for age is",quantile(df$`Age`))

cat("The Quantile for male is",quantile(df$`Male`))


```

### Summary
```{r}
summary(df)
```




## *** Univariate***
```{r}
frequency <- table(df$Male)
frequency
barplot(frequency,col=c("Cyan","lightgreen"),main="Barchart for Male",xlab = "Male",ylab = "Total Count")
```

```{r}
frequency <- table(df$Clicked.on.Ad)
frequency
barplot(frequency,col=c("green","purple"),main="Barchart for Clicked on Ad",xlab = "Clicked on Add",ylab = "Total Count")
```

```{r}
frequency <- table(df$Age)
frequency
barplot(frequency,main="Barchart for Age",xlab = "Age",ylab = "Total Count")
```

### ***Histograms***

```{r}
library("ggplot2")
```
```{r}
ggplot(df, aes(Daily.Internet.Usage)) + geom_histogram(bins = 20, color = 'cyan') + 
    labs(title = 'Daily internet usage distribution', x = 'Daily internet usage', y = 'Frequency')
```
There is no particular pattern for the daily internet usage

```{r}
ggplot(df, aes(Area.Income)) + geom_histogram(bins = 20, color = 'cyan') + 
    labs(title = 'Income distribution', x = 'Income', y = 'Frequency')
```
Most People were earning between 50,000 and 70,000.

```{r}
ggplot(df, aes(Daily.Time.Spent.on.Site)) + geom_histogram(bins = 20, color = 'cyan') + 
    labs(title = 'Daily time spent on site distribution', x = 'Daily time spent on site', y = 'Frequency')
```
Majority of the people spent about 60-80 minutes on the blog site.

```{r}
ggplot(df, aes(Age)) + geom_histogram(bins = 20, color = 'cyan') + 
    labs(title = 'Age distribution', x = 'Age', y = 'Frequency')
```
Most of the respondents are between the ages of 25 and 50.

### Bivariate  analysis
 #### covariance
```{r}
covarince <- cov(df[,sapply(df,is.numeric)])
covarince
```
### correlation  of all numeric variable
```{r}
df.cor = cor(df[,sapply(df,is.numeric)],method = c('spearman'))
df.cor
```

```{r}
library(corrplot)
```

#### Correlation plot
```{r}
corrplot(df.cor,method="number",main="Correlatio of numerical variables")
```
#### Heatmap
```{r}
heatmap(x=df.cor)
```

###scatterplots```

#Scatter plot for area in income vs daily internet usage

```{r}
ggplot(df, aes(x = Area.Income, y = Daily.Internet.Usage )) +
  geom_point() + labs(title = 'Scatter plot for area in income vs daily internet usage')
```

```{r}
#Scatter plot for age vs daily time spent on site
ggplot(df, aes(x = Age, y = Daily.Time.Spent.on.Site)) +
  geom_point() + labs(title = 'Scatter plot for age vs daily time spent on site')
```

Most people who spend time on site are between ages of 30-50




```{r}
# Scatter plot of internet usage 
ggplot(df, aes(x =Daily.Internet.Usage, y = Daily.Time.Spent.on.Site,color = Clicked.on.Ad)) +geom_point()+geom_smooth()+ labs(title = 'Scatter plot age vs area in income')
```
The time spent by the respondents on site does not necessarily translate to the respondents clicking on ads.
```{r}
df %>% group_by(Country, Daily.Internet.Usage)%>% head(10)%>% arrange(desc(Daily.Internet.Usage))
```

```{r}
df %>% group_by(Country, Daily.Time.Spent.on.Site)%>% head(10)%>% arrange(desc(Daily.Time.Spent.on.Site))
```


## ** Modelling**
```{r}
#library(tidyverse)
```

```{r}
#writeLines('PATH="${RTOOLS40_HOME}\\usr\\bin;${PATH}"', con = "~/.Renviron")


```
```{r}
#install.packages("superml",dependencies = TRUE, repos = 'http://cran.rstudio.com/')

```

```{r}
library(superml)
print("Data before label encoding..\n")
label <- LabelEncoder$new()
#Label encoding
df$City <- label$fit_transform(df$City)
#print(df$City)
df$Country <- label$fit_transform(df$Country)
#print(df$Country)
df$Ad.Topic.Line <- label$fit_transform(df$Ad.Topic.Line)
#print(df$Ad.Topic.Line)
```
### ** Supervised learning**
### KNN
```{r}
df2 <- subset(df, select = c(Daily.Time.Spent.on.Site,Age,Area.Income,Daily.Internet.Usage,Ad.Topic.Line,City,Male,Country,Clicked.on.Ad))
names(df2)

```
```{r}
set.seed(1234)
# Randomizing the rows, creates a uniform distribution of 150
random <- runif(150)
df_random <- df2[order(random),]
# Selecting the first 6 rows from iris_random
head(df_random)
```
```{r}
normal <- function(x) (
  return( ((x - min(x)) /(max(x)-min(x))) )
)
normal(1:9)
df_new <- as.data.frame(lapply(df_random[,-9], normal))
summary(df_new)
```
```{r}
# Lets now create test and train data sets
train <- df_new[1:130,]
test <- df_new[131:150,]
train_sp <- df_random[1:130,9]
test_sp <- df_random[131:150,9]
```

```{r}
# Lets build a model on it; cl is the class of the training data set and k is the no of neighbours to look for 
# in order to classify it accordingly
library(class)  
require(class)
model <- knn(train= train,test=test, cl= train_sp,k=13)
table(factor(model))
fd<-table(test_sp,model)
fd

```

```{r}
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(fd)

```
Knn has an accuracy score of 95%

#### *** SVM**


```{r}
library(caret)

```

```{r}
# So, 70% of the data is used for training and the remaining 30% is #for testing the model.
# - The “list” parameter is for whether to return a list or matrix. 
# We are passing FALSE for not returning a list
# 
intrain <- createDataPartition(y = df2$Clicked.on.Ad, p= 0.7, list = FALSE)
training <- df2[intrain,]
testing <- df2[-intrain,]
```
```{r}
dim(training); 
dim(testing)
```

```{r}
#Changing our target variable to factor
training[["Clicked.on.Ad"]] = factor(training[["Clicked.on.Ad"]])
```

```{r}
# The trainControl method will take three parameters:
# a) The “method” parameter defines the resampling method, 
# in this demo we’ll be using the repeatedcv or the repeated cross-validation method.
# b) The next parameter is the “number”, this basically holds the number of resampling iterations.
# c) The “repeats ” parameter contains the sets to compute for our repeated cross-validation. 
# We are using setting number =10 and repeats =3
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
svm_Linear <- train(Clicked.on.Ad ~., data = training, method = "svmLinear",
trControl=trctrl,
preProcess = c("center", "scale"),
tuneLength = 10)
```

```{r}
svm_Linear
```
```{r}
# We can use the predict() method for predicting results as shown below. 
# We pass 2 arguements, our trained model and our testing data frame.
# ---
# 
test_pred <- predict(svm_Linear, newdata = testing)
test_pred
```
```{r}
# Now checking for our accuracy of our model by using a confusion matrix 
# ---
# 
confusionMatrix(table(test_pred, testing$Clicked.on.Ad))
```
SVM linear has an accuracy score of 97%

#### ** Naives bayes**
```{r}
library(tidyverse)
library(ggplot2)
library(caret)#confusionMatrix
library(caretEnsemble)
library(psych)
library(Amelia)#missmap
library(mice) #mice
library(GGally) #ggpairs
library(rpart)
library(randomForest)
library(tidyverse)
```

```{r}
# describing the data
summary(df2)
```
```{r}
# We convert the output variable into a categorical variable
df2$Clicked.on.Ad <- factor(df2$Clicked.on.Ad)
df2$Clicked.on.Ad
```
```{r}
# We visualize our dataset by checking how many missing values
missmap(df2)
```
```{r}
# Splitting data into training and test data sets
# ---
# 
indxTrain <- caret::createDataPartition(y = df2$Clicked.on.Ad,p = 0.75,list = FALSE)
training <- df2[indxTrain,]
testing1 <- df2[-indxTrain,]
```

```{r}
# Checking dimensions of the split
prop.table(table(df2$Clicked.on.Ad)) * 100
prop.table(table(df2$Clicked.on.Ad)) * 100
prop.table(table(df2$Clicked.on.Ad)) * 100
```
```{r}
# Comparing the clicked on add of the training and testing phase
# Creating objects x which holds the predictor variables and y which holds the response variables
# ---
#
x = training[,-9]
colSums(is.na(x))
y = training$Clicked.on.Ad
```
```{r}
# Loading our inbuilt e1071 package that holds the Naive Bayes function.
library(e1071)
```

```{r}
# Now building our model 
model = train(x,y,'nb',trControl=trainControl(method='cv',number=10))

```

```{r}
# Model Evalution
# Predicting our testing set
# 
Predict <- predict(model,newdata = testing1 )

```
```{r}
# Getting the confusion matrix to see accuracy value and other parameter values
confusionMatrix(Predict, testing1$Clicked.on.Ad)
```
 Naive bayes has an accuracy score of 95%
 
 #### ** Decision trees***
```{r}
set.seed(100)
id<- sample(2,nrow(df2),prob =c(0.7,0.3),replace = T)
traindf<- df2[id==1,]
testdf<- df2[id==2,]
```
 
```{r}
library(rpart)
library(rpart.plot)
m <- rpart(Clicked.on.Ad ~., data = df2,
           method = "class")
rpart.plot(m)
```
```{r}
p <- predict(m,df2, type = "class") 
cm <- table(p,df$Clicked.on.Ad)
cm
```
```{r}
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(cm)
```
 Decision tree has an accuracy score of 96%
### ** Hyperparameter turning for decision trees**

Training the decision tree model
```{r}
df$Timestamp<- NULL
names(df)
```
```{r}
names(df)
make.names(names(df))
colnames(df) <- make.names(colnames(df),unique = T)
```
```{r}
#install.packages("mlr",,dependencies = TRUE, repos = 'http://cran.rstudio.com/')
library(mlr)
dfTask <- makeClassifTask(data = df, target = "Clicked.on.Ad")
tree <- makeLearner("classif.rpart")
```

```{r}
# Printing available rpart hyperparameters
ls()
getParamSet(tree)
```
### Defining the hyperparameter space for tuning
```{r}
treeParamSpace <- makeParamSet(
  makeIntegerParam("minsplit", lower = 5, upper = 20),
  makeIntegerParam("minbucket", lower = 3, upper = 10),
  makeNumericParam("cp", lower = 0.01, upper = 0.1),
  makeIntegerParam("maxdepth", lower = 3, upper = 10))

```
```{r}
# Defining the random search
randSearch <- makeTuneControlRandom(maxit = 200)
cvForTuning <- makeResampleDesc("CV", iters = 5)

```

### Performing hyperparameter
```{r}
install.packages("detector",dependencies = TRUE,repos = 'http://cran.rstudio.com/')

library(parallelMap)
library(detector)
library(parallel)
parallelStartSocket(cpus = detectCores())
tunedTreePars <- tuneParams(tree, task = dfTask,
                            resampling = cvForTuning,
                            par.set = treeParamSpace,
                            control = randSearch)
parallelStop()

```
### Training the model with the tuned hyperparameters
```{r}
# Training the final tuned model
tunedTree <- setHyperPars(tree, par.vals = tunedTreePars$x)
tunedTreeModel <- train(tunedTree, dfTask)
```

```{r}
treeModelData <- getLearnerModel(tunedTreeModel)
rpart.plot(treeModelData, roundint = FALSE,
box.palette = "BuBn",
type = 5)
```
#### Exploring the model
```{r}
printcp(treeModelData, digits = 3)

```
```{r}
# Cross-validating the model-building process
outer <- makeResampleDesc("CV", iters = 5)
treeWrapper <- makeTuneWrapper("classif.rpart", resampling = cvForTuning,
                               par.set = treeParamSpace,
                               control = randSearch)
parallelStartSocket(cpus = detectCores())
cvWithTuning <- resample(treeWrapper,dfTask, resampling = outer)
parallelStop()
```

```{r}
# Extracting the cross-validation result
cvWithTuning

```
#### ** Random Forest**
```{r}
library(randomForest)
dfforest <- randomForest(Clicked.on.Ad ~ Daily.Time.Spent.on.Site+Age+
             Area.Income+Daily.Internet.Usage+Ad.Topic.Line+
             City+Country+Male,data=traindf)
dfforest

```

```{r}
predforest <- predict(dfforest,testdf,type="class")
predforest

```

```{r}
confusionMatrix(table(predforest,testdf$Clicked.on.Ad))

```

```{r}
ranTree=table(predforest,testdf$Clicked.on.Ad)
ranTree

```

```{r}
accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(ranTree)

```

```{r}
importance(dfforest)
```

```{r}
important <- importance(dfforest)
important
Important_Features <- data.frame(Feature = row.names(important), Importance = important[, 1])
Important_Features

```
```{r}
plot_ <- ggplot(Important_Features, 
    aes(x= reorder(Feature,
Importance) , y = Importance) ) +
geom_bar(stat = "identity", 
        fill = "#800080") +
coord_flip() +
theme_light(base_size = 20) +
xlab("") + 
ylab("Importance")+
ggtitle("Important Features in Random Forest\n") +
theme(plot.title = element_text(size=18))
ggsave("important_features.png", 
      plot_)
plot_
```
## ** Conclusions***
1.) Knn  model has an accuracy score of 95%,svm has an accuracy score of 97% ,naive bayes has an accuracy score of 95% , decision tree  has an accuracy score of 96%
2.) The age  between of 28  and 48 record the highest ad click on the site
3.) Tunisia ,Italy and san marino are the 3 top countries with the highest internet usage
4.) Myanmar,Nauru and Grenad spend the most time on the site

## **Recommendations**
* Svm  should be the best method to be used for comparison
* The ads posted on the client site should be more relevant to this demographic between late twenties and early forties



